#  ğŸ‘¨â€ğŸ’»è¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œä½¿ç”¨è¯åºè¿›è¡Œæ–‡æœ¬åˆ†ç±»

## ğŸ’¡å…³äº

æœ¬é¡¹ç›®ä½¿ç”¨ python 3.10 å’Œ PyTorch 0.4.1 åˆ›å»º

## ğŸ“œæ•°æ®é›†å’Œä½¿ç”¨æ¨¡å‹

- [Quest_CNN](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/quest_cnn)ï¼šç”¨äºæ–‡æœ¬æ•°æ®ä¸­é—®é¢˜è¯†åˆ«çš„å¤šé€šé“æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ
- [KIM_CNN](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/kim_cnn)ï¼š ç”¨äºå¥å­åˆ†ç±»çš„å·ç§¯ç¥ç»ç½‘ç»œ
- [XML_CNN](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/xml_cnn)ï¼šæç«¯å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»çš„æ·±åº¦å­¦ä¹ 
- [Seq_cnn](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/seq_cnn)ï¼šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œæœ‰æ•ˆä½¿ç”¨è¯åºè¿›è¡Œæ–‡æœ¬åˆ†ç±»
- [FastText](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/FastText)ï¼šé«˜æ•ˆæ–‡æœ¬åˆ†ç±»çš„æŠ€å·§åŒ…
- [CHAR_CNN](https://github.com/Xuyan-cmd/Effective-Use-of-Word-Order-for-Text-Categorization/tree/main/neural_network/char_cnn) :å­—ç¬¦çº§å·ç§¯ç½‘ç»œ

å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œæˆ‘åœ¨æ–‡ä»¶å¤¹ä¸­æä¾›äº†é¢å¤–çš„è‡ªè¿°æ–‡ä»¶ï¼ŒåŒ…å«æœ‰å…³å¦‚ä½•è¿è¡Œæ¯ä¸ªæ¨¡å‹çš„è¯´æ˜

## ğŸš€å¿«é€Ÿä¸Šæ‰‹

### æç¤º

å»ºè®®åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…å’Œè¿è¡Œä»£ç ã€‚

### åˆ›å»º Conda è™šæ‹Ÿç¯å¢ƒ

é¦–å…ˆï¼Œä»æ­¤[é“¾æ¥ä¸‹è½½ Anaconda](https://www.anaconda.com/distribution/)

å…¶æ¬¡ï¼Œç”¨python 3.10åˆ›å»ºä¸€ä¸ªcondaç¯å¢ƒã€‚

```javascript
$ conda create -n cnn37 python=3.10
```
é‡æ–°å¯åŠ¨ç»ˆç«¯ä¼šè¯åï¼Œä½ å¯ä»¥æ¿€æ´» conda ç¯å¢ƒï¼š
```javascript
$ conda activate cnn37
```
### å®‰è£…æ‰€éœ€çš„pythonåŒ…

åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€çš„åŒ…ã€‚

```javascript
pip install -r requirements.txt
```

æœ€åï¼Œéœ€è¦ä¸‹è½½ NLTK åº“ä¸­çš„åœç”¨è¯ï¼š

```javascript
python
import nltk
nltk.download()
```

### ä¸‹è½½é¢„è®­ç»ƒåµŒå…¥

**è°·æ­Œé¢„è®­ç»ƒåµŒå…¥**

ä¸ºäº†å¯¹è¯åµŒå…¥ï¼ˆæˆ–è¯­ä¹‰åµŒå…¥ï¼‰ä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥ï¼Œéœ€è¦å°† `GoogleNews-vectors-negative300.bin.gz` ä¸‹è½½åˆ°æ–‡ä»¶å¤¹*`embedding_input/google_embedding`*

```
wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
```

### å‚æ•°è°ƒæ•´

ä¸ºäº†è°ƒæ•´æ¯ä¸ªæ¨¡å‹çš„è¶…å‚æ•°ï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ª json æ–‡ä»¶ï¼Œä¾‹å¦‚*`search_spaces/cnn.json`*æ–‡ä»¶å¹¶å°†å…¶æ·»åŠ åˆ°*`search_spaces/`*

ä¹‹åï¼Œè¿è¡Œï¼š

```javascript
python3 param_json.py --model_name "model_name"  -fn "results_file_name" - -jf "search_spaces/model.json" -st search_trials
```

æœ€ç»ˆç»“æœå°†ä¿å­˜åœ¨*dataset_output/hyperpameters/*ä¸­ï¼Œå®ƒå°†åˆ›å»ºä¸‰ä¸ªæ–‡ä»¶ï¼š

- `results_file_name.csv` ï¼šåŒ…å«æ¯ä¸ªæœç´¢è¯•éªŒçš„æ‰€æœ‰æœ€ç»ˆ F1 åˆ†æ•°
- `results_file_name.json` ï¼šåŒ…å«æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°
- `results_file_name_param.csv` ï¼šæ¨¡å‹çš„å‚æ•°æ•°é‡

ä¹‹åï¼Œè¿è¡Œï¼š

```javascript
python3 main_iterations.py --model_name "model_name"  -fn "results_file_name"
```

æœ€ç»ˆç»“æœå°†ä¿å­˜åœ¨*`dataset_output/results/`*ä¸­ï¼Œå®ƒå°†åˆ›å»ºä¸¤ä¸ªæ–‡ä»¶ï¼š

- `results_file_name.csv` ï¼šåŒ…å«æ¯ç§å­çš„æ‰€æœ‰ç»“æœï¼Œæµ‹è¯•é›†çš„å‡å€¼å’Œæ ‡å‡†å·®
- `results_file_name_val.csv` ï¼šåŒ…å«æ¯ç§å­çš„æ‰€æœ‰ç»“æœï¼Œè¯•éªŒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®

ä¸ºäº†æŸ¥çœ‹å¯ä»¥æ›´æ”¹çš„æ‰€æœ‰å‚æ•°ä»¥è¿›è¡Œå…¶ä»–å®éªŒï¼š

```javascript
python main_iterations.py -help
 

usage: main_iterations.py [-h] [-modn MODEL_NAME] [-fn DATA_FINAL_NAME]
                          [-dn DATA_NAME] [-ner NER] [-df DATA_FILE]
                          [-dft DATA_FILE_TEST] [-dd DATA_FILE_DEV]
                          [-e EMBD_FILE] [-e_mimic EMBD_FILE_MIMIC]
                          [-e_flag EMBEDDING_FLAG] [-cn CLASS_NUMBER]
                          [-tr TRAINING_RATIO] [-tv TEST_VAL_RATIO]
                          [-l EMBEDDING_SIZE] [-opt OPTIM] [-b BATCH_SIZE]
                          [-n NUM_ITERS] [-lr LEARNING_RATE]
                          [-wd WEIGHT_DECAY] [-usemb USE_EMBEDDING]
                          [-tr_e TRAINING_EMBEDDING] [-sp SAVE_PATH]
                          [-pr PRINTING_LOSS] [-multi KMULTICHANNEL]
                          [-dr DROPOUT] [-fm FEATURE_MAPS]
                          [-fs [FILTER_SIZES [FILTER_SIZES ...]]]
                          [-z HIDDEN_SIZE] [-qm QUESTION_NAME]
                          [-qml QUESTION_NAME_LABEL]

optional arguments:
  -h, --help            show this help message and exit
  -modn MODEL_NAME, --model_name MODEL_NAME
                        name of the anmodel we are using
  -fn DATA_FINAL_NAME, --data_final_name DATA_FINAL_NAME
                        result name.
  -dn DATA_NAME, --data_name DATA_NAME
                        Dataset name.
  -ner NER, --ner NER   whether we use ner or re task
  -df DATA_FILE, --data_file DATA_FILE
                        Path to dataset.
  -dft DATA_FILE_TEST, --data_file_test DATA_FILE_TEST
                        Path to dataset test set.
  -dd DATA_FILE_DEV, --data_file_dev DATA_FILE_DEV
                        Path to dataset.
  -e EMBD_FILE, --embd_file EMBD_FILE
                        Path to Embedding File of google.
  -e_mimic EMBD_FILE_MIMIC, --embd_file_mimic EMBD_FILE_MIMIC
                        Path to Embedding File of mimic.
  -e_flag EMBEDDING_FLAG, --embedding_flag EMBEDDING_FLAG
                        1 use google embedding, 2 use mimic dataset, 3 random
                        start
  -cn CLASS_NUMBER, --class_number CLASS_NUMBER
                        Number of class
  -tr TRAINING_RATIO, --training_ratio TRAINING_RATIO
                        Ratio of training set.
  -tv TEST_VAL_RATIO, --test_val_ratio TEST_VAL_RATIO
                        Ratio of testing/validation set.
  -l EMBEDDING_SIZE, --embedding_size EMBEDDING_SIZE
                        embedding size
  -opt OPTIM, --optim OPTIM
                        optimization
  -b BATCH_SIZE, --batch_size BATCH_SIZE
                        Batch Size.
  -n NUM_ITERS, --num_iters NUM_ITERS
                        Number of iterations/epochs.
  -lr LEARNING_RATE, --learning_rate LEARNING_RATE
                        Learning rate for optimizer.
  -wd WEIGHT_DECAY, --weight_decay WEIGHT_DECAY
                        weight decay
  -usemb USE_EMBEDDING, --use_embedding USE_EMBEDDING
                        if we use pre-training embedding
  -tr_e TRAINING_EMBEDDING, --training_embedding TRAINING_EMBEDDING
                        If we will continue the training of embedding.
  -sp SAVE_PATH, --save_path SAVE_PATH
                        path where the model will be saved
  -pr PRINTING_LOSS, --printing_loss PRINTING_LOSS
                        whether we print the training loss in each epoch
  -multi KMULTICHANNEL, --kmultichannel KMULTICHANNEL
                        whether we use mutlichannel for Kim
  -dr DROPOUT, --dropout DROPOUT
                        dropout for cnn_text
  -fm FEATURE_MAPS, --feature_maps FEATURE_MAPS
                        size of feature map for each filter
  -fs [FILTER_SIZES [FILTER_SIZES ...]], --filter_sizes [FILTER_SIZES [FILTER_SIZES ...]]
                        size for each filter
  -z HIDDEN_SIZE, --hidden_size HIDDEN_SIZE
                        Number of Units in LSTM layer.
  -qm QUESTION_NAME, --question_name QUESTION_NAME
                        name of the column that contain questions
  -qml QUESTION_NAME_LABEL, --question_name_label QUESTION_NAME_LABEL
```








